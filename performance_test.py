#!/usr/bin/env python3
"""
Performance test script for the optimized Turkish lemmatizer
Tests batch processing, caching, and parallel execution
"""

import asyncio
import aiohttp
import time
import json
from typing import List

# Test data - realistic Turkish n-grams
TEST_DATA = [
    "merkezi sistem yÃ¶netimi",
    "idari yaptÄ±rÄ±m uygulamasÄ±", 
    "kanuni faiz hesaplamasÄ±",
    "hukuki sÃ¼reÃ§ takibi",
    "mali sorumluluk sigortasÄ±",
    "ticari iÅŸlem kayÄ±tlarÄ±",
    "resmi belge dÃ¼zenleme",
    "kamu hizmet sunumu",
    "sosyal gÃ¼venlik sistemi",
    "eÄŸitim Ã¶ÄŸretim faaliyetleri",
    "saÄŸlÄ±k hizmetleri yÃ¶netimi",
    "Ã§evre koruma Ã¶nlemleri",
    "teknoloji geliÅŸtirme projesi",
    "araÅŸtÄ±rma geliÅŸtirme faaliyeti",
    "kalite kontrol sÃ¼reci",
    "gÃ¼venlik Ã¶nlemleri uygulamasÄ±",
    "performans deÄŸerlendirme sistemi",
    "stratejik planlama sÃ¼reci",
    "risk yÃ¶netimi uygulamasÄ±",
    "sÃ¼rekli iyileÅŸtirme faaliyeti"
]

# Generate large test dataset
def generate_large_dataset(base_data: List[str], multiplier: int = 100) -> List[str]:
    """Generate a large dataset by repeating base data"""
    large_dataset = []
    for i in range(multiplier):
        for item in base_data:
            # Add some variation to test caching
            variation = f" {i}" if i % 10 == 0 else ""
            large_dataset.append(item + variation)
    return large_dataset

async def test_performance(session: aiohttp.ClientSession, texts: List[str], test_name: str):
    """Test performance for a given dataset"""
    print(f"\nğŸ§ª {test_name}")
    print(f"ğŸ“Š Testing with {len(texts)} texts...")
    
    payload = {"texts": texts, "return_details": True}
    
    start_time = time.time()
    
    try:
        async with session.post('http://localhost:8077/lemmatize', json=payload) as response:
            if response.status == 200:
                result = await response.json()
                processing_time = (time.time() - start_time) * 1000
                
                # Extract performance metrics
                perf_data = result.get('performance', {})
                api_time = perf_data.get('processing_time_ms', 0)
                cache_info = perf_data.get('cache_hits')
                
                print(f"âœ… Success!")
                print(f"â±ï¸  Total time: {processing_time:.1f}ms")
                print(f"ğŸ”§ API processing time: {api_time}ms")
                
                # Calculate texts per second safely
                if api_time > 0:
                    texts_per_sec = len(texts) / (api_time / 1000)
                    print(f"ğŸ“ˆ Texts per second: {texts_per_sec:.1f}")
                else:
                    print(f"ğŸ“ˆ Texts per second: N/A (API time = 0)")
                
                if cache_info:
                    print(f"ğŸ’¾ Cache info: {cache_info}")
                    
                # Show first few results
                lemmas = result.get('lemmas', [])
                print(f"ğŸ“ First 3 results:")
                for i, lemma in enumerate(lemmas[:3]):
                    print(f"   {i+1}. '{texts[i]}' â†’ '{lemma}'")
                    
            else:
                print(f"âŒ Error: {response.status}")
                error_text = await response.text()
                print(f"   {error_text}")
                
    except Exception as e:
        print(f"âŒ Exception: {e}")

async def run_performance_tests():
    """Run comprehensive performance tests"""
    print("ğŸš€ Turkish Lemmatizer Performance Tests")
    print("=" * 50)
    
    # Test different scenarios
    test_scenarios = [
        (TEST_DATA[:1], "Single Text Test"),
        (TEST_DATA[:5], "Small Batch Test (5 texts)"),
        (TEST_DATA, "Medium Batch Test (20 texts)"),
        (generate_large_dataset(TEST_DATA, 10), "Large Batch Test (200 texts)"),
        (generate_large_dataset(TEST_DATA, 50), "Very Large Batch Test (1000 texts)"),
        (generate_large_dataset(TEST_DATA, 100), "Massive Batch Test (2000 texts)"),
    ]
    
    async with aiohttp.ClientSession() as session:
        # Test service health first
        try:
            async with session.get('http://localhost:8077/health') as response:
                if response.status == 200:
                    print("âœ… Service is healthy")
                else:
                    print("âŒ Service health check failed")
                    return
        except Exception as e:
            print(f"âŒ Cannot connect to service: {e}")
            print("ğŸ’¡ Make sure the service is running on http://localhost:8077")
            return
        
        # Run performance tests
        for texts, test_name in test_scenarios:
            await test_performance(session, texts, test_name)
            
        # Test caching effectiveness
        print(f"\nğŸ”„ Cache Effectiveness Test")
        print("Testing same data twice to measure cache hits...")
        
        cache_test_data = TEST_DATA[:10]
        
        # First run (cold cache)
        await test_performance(session, cache_test_data, "Cache Test - First Run")
        
        # Second run (warm cache)
        await test_performance(session, cache_test_data, "Cache Test - Second Run")

def print_performance_recommendations():
    """Print performance recommendations"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ PERFORMANCE RECOMMENDATIONS")
    print("=" * 60)
    
    print("\nğŸ¯ Optimization Strategies Implemented:")
    print("   âœ… Batch Processing: Texts processed in groups of 500")
    print("   âœ… Parallel Execution: Up to 4 concurrent workers")
    print("   âœ… LRU Caching: 10,000 most recent lemmatizations cached")
    print("   âœ… Async Processing: Non-blocking I/O operations")
    print("   âœ… Smart Routing: Different strategies based on input size")
    
    print("\nâš¡ Expected Performance Improvements:")
    print("   ğŸ“ˆ 3-5x faster for repeated text patterns (cache hits)")
    print("   ğŸ“ˆ 2-3x faster for large batches (parallel processing)")
    print("   ğŸ“ˆ Better resource utilization (async + threading)")
    print("   ğŸ“ˆ Reduced memory overhead (batch processing)")
    
    print("\nğŸ”§ Configuration Options:")
    print("   â€¢ BATCH_SIZE = 500 (adjust based on memory)")
    print("   â€¢ MAX_WORKERS = 4 (adjust based on CPU cores)")
    print("   â€¢ CACHE_SIZE = 10000 (adjust based on available memory)")
    
    print("\nğŸ’¡ Usage Tips:")
    print("   â€¢ Send multiple texts in single request for best performance")
    print("   â€¢ Use consistent text patterns to maximize cache hits")
    print("   â€¢ Monitor cache hit rates in response metrics")
    print("   â€¢ Consider increasing workers for CPU-bound workloads")

if __name__ == "__main__":
    print("ğŸ”§ Starting performance tests...")
    print("ğŸ’¡ Make sure the lemmatizer service is running!")
    print("   Run: python app.py or docker-compose up")
    print()
    
    try:
        asyncio.run(run_performance_tests())
        print_performance_recommendations()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Tests interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
